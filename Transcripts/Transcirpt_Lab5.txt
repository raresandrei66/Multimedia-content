Transcriere


0:00
all right welcome everybody to this new laboratory so uh in the laboratory of
0:05
today we will talk about photo response nonuniformity why for the response
0:11
nonuniformity well in the first part of the multimedia data security curse we talk about digital water markeing we
0:18
then learn how we can embed in an invisible and robust way a watermark
0:25
into an image and we can use it for many task such as data protection so or also
0:33
um tampering detection data protection in the case we want to protect our image
0:39
um and so well the copyright of our image we want to state that we uh are
0:45
actually the author of um either the image or the image plus postprocessing
0:51
of this letter and then um temperary detection because we learned that there
0:56
were uh many watermarking techniques um that are actually invisible but then
1:03
they're also fragile and that they are very suitable for this task meaning that if we take an image we embed a very
1:09
invisible but fragile water marking and then in the moment that someone try to
1:15
uh manipulate this image applying a different postprocessing techniques they are just going to remove our Watermark
1:23
and um then maybe uh if we are able to retrieve this image and we know that
1:29
it's our image but uh we are not able to retrieve our Watermark then we can say
1:34
that the image was modified and so was Ted in any possible way now um we learn all these things
1:42
about water marking and today we are going to talk about watero response non uniformity which somehow is also
1:48
defining literature has a natural water marking why a natural water marking well because photor response uniformity is a
1:56
very um weak signal noise rece idual signal
2:01
introduced by all our devices either iPhones smartphones sorry uh tablet or
2:09
digital camera specifically by the sensor of the cameras of our devices
2:16
every time we take an image and how so well um it's strictly related with the
2:23
camera sensor of our devices in the specifically with its manufacturing process um meaning that it depends by
2:32
the temperature humidity dust level how it's been touched processed and
2:39
manufactured um during the manufacturing process of the sensor so all these little pecularities uh uh generates um
2:48
makes so that uh the photoresponse um non uniformity or the p is no residual
2:54
that is somehow unique for every devices and by being unique is uh it can
3:01
be used as some sort of um Watermark has in the case we saw in the previous in
3:08
the previous lry so this is just an overview but now we're going through every um details of the photo response
3:15
non uniformity we are going through also its problems possible solutions and then
3:21
we are going to see um how it's going in the real world so if this photo response
3:26
non uniformity can still be used in the real world when it has it's um it can be
3:32
used and um what we are doing in order to make it more V uh on real work Cas SC
3:40
so to start photo response nonuniformity um as I've wrote here
3:47
photo respon nonuniformity is some sort of natural digital water marketing why it's natural because we never embedd it
3:55
as we did for I don't know the Spectrum the LSP or the or the
4:00
um DWT uh spectum techniques or other watermarking techniques but there
4:06
something introduced by the sensor of our camera devices he their cameras smartphones tablet and so on so
4:13
forth um and how well um as I said is a scun residual
4:20
uh a noise Source introduced by the sensor of our device every time we take a pictures or record a video and
4:27
specifically every time a light source is captured by our camera sensor why T
4:33
to resemble a watermark well because the pru is unique um has the watermark you
4:39
embed in our um first uh first class on digital water
4:45
marketing and um it is unique it means that for each device and not model
4:51
corresponds a unique pru so if we have multiple iPhone 13s my iPhone 13s will
4:57
have a different per new respect to the iPhone 13 of another another person
5:03
similarly if we have multiple Canon M6 then the p u of my Canon M6 will be uh
5:10
different with respect to the Canon M6 of another person and this is because as I uh introduced uh during the first part
5:17
of this class it depends by the manufacturing process of the camera sensor meaning that condition of
5:25
temperature humidity dust and manufacturing of this sensor make it
5:31
possible that the p u of my camera is different from p u of another camera of
5:38
the same mole now uh this p u
5:43
um is introduced on our image as I said before every time that we take an image
5:50
or a video because is a noise Source a noise Source generated by the sensor
5:56
then these noise Source can be separated from the whole image and then can be
6:02
extracted from an image and once that we extract the pnu from let's say image a
6:07
we can compare it with the p of image B and if we have a large value of correlation as we will see in the rest
6:13
of these lectures then it means that they were the image a and image B were
6:18
taken with the same camera device otherwise if they ply correlate in that
6:24
case we know that Al least in theory we will see that is not always like that and we have to develop techniques to
6:30
overcome these issues but um if image a well the pru exract from image a and the
6:36
prnu uh extract from image B poly correlate it means that they were not taken from the same device and by the
6:42
way with pru um I specified even if I have written over here uh I means photor
6:50
response nonuniformity then why is the pru unique
6:56
we say the pru uniqueness seems to be related I say seems because there are some studies but I mean there are
7:03
studies proven uh but they all became from conector um the pru uniqueness
7:10
seems to be related to the camera sensor manufacturing process specifically condition of dust humidity temperature
7:17
and crafting of the sensor produce slightly different results you know slightly different sensors slightly
7:23
different sensor capture the light in a slightly different way let's say microscopically they capture the light
7:29
in the same way but microscopically they do not um as a consequence the
7:35
functioning of each camera sensor is lightly different and the pru uh as
7:41
well and then one say well cool then why are we watermarking images because uh
7:48
the pru is invisible but not robust meaning that if you still want to protect the copyright of your image and
7:55
the property of the image you take is better if you rely on more robust water marking techniques we already saw during
8:02
the first three lecture that these are not um always robust they cannot be
8:07
always robust but the pru is a very well I will say not bad way but not
8:15
um smart way to protect the copyright of your image but we will see even later
8:22
especially with new devices um the raise that we had a raise of case of false
8:28
positive when we use the pr so if you want to protect the property of your image if you want to state that that
8:34
image was taken by you processed by you whatever just in by The
8:39
Watermark and now since I say that it resemble a watermarking techniques but
8:44
is not a water Market techniques you may ask then why I when I'm supposed to use
8:50
the pru and well we have different cases in which we can use the p u um we can
8:56
use it to identify inde the Sear so the device used take an image and these are
9:02
for cases while cult cases in which um an image was taken without a consent of
9:09
a person we may um there may be for instance quter cases of privacy
9:15
violations CH pornography and so on so forth um in these specific cases what
9:22
happens is that we have an image taken without the consent of a person so this
9:28
person identify the image and so that this image has been spread without his consent go to the police and the police
9:34
starts on investigations the police then um let's
9:40
say file a list of suspect and confiscated the devices so
9:46
smartphone tablet camera devices DSLR camera whatever from this
9:53
suspect and they use this camera uh hopefully to take a uh set of image
10:00
which is of this device and extract the pru from this set of image um taken with
10:07
each device of each suspect and then they extract the pru
10:13
also from the image that is the problems in privacy relation so the image taken
10:19
without the consent of the person who file um well who went to the police and
10:25
say well someone took this picture of me without my consent then the point is that using the image
10:33
well using the p uh instruct from the image of the confiscated devices they
10:39
can compose the so-called camera fingerprint for each of these devices um why we call the camera
10:46
fingerprint because let's say that is like the Fingerprints of your thumb or of your finger so it's uh somehow
10:53
identify that specific device and then we can compare that camera fingerprint with the p stru from the image that is
11:01
let's say the image um the the image resembling the
11:07
problems in privac relations compare the P of the image under analysis with the
11:13
camera fingerprint of each of these devices and if we have at least one devices of which correlation is very
11:18
high then we know that this image was taken with that specific devices that is property of that specific suspect and we
11:26
can let's say proceed um well interrogating the suspect and then well
11:32
all the Cod ter process can go now it may be not very clear right
11:38
now but just because we haven't seen any pictures depicting this specific case
11:44
called Source attributions that is one of the specific problems on which pru is
11:50
involved now instead we have a pictures let's say summarizing what I just said so as I said we have an image
11:58
here uh um that we believe was taken with the device of these pictures so we
12:05
take this image we extract this pru and here we have the pru representation of
12:10
our image then we have the device we suspect was used to take this image with these
12:17
devices we take another set of images and from each of these images we exract
12:23
the pnu and we compose the camera fingerprint how we compose the camera fingerprint by basically averaging
12:29
together together the pru of each of these image if then the correlation between the pru of the image and their
12:35
analysis and the prnu of the um well and the and the camera fingerprint expose a
12:42
large correlation then we can say that the two image were taken with the same device and we can proclaim a match in
12:49
the opposite cases uh we may believe that this image was taken with these
12:54
camera devices but instead was taken with the smartphone and what happens is that
12:59
we're going to extract the pru and then we are going to compare it as we uh said before with a camera fingerprint of
13:06
these devices but we will have a pool correlations in this case we can say that we have a mismatch meaning that we
13:13
know for sure that this image was not taken with the spe the camera fingerprint of the device we are
13:21
testing now the question may be all right it may be somehow clear what is
13:26
the p u when it um it can be used and um
13:33
that we need to extract them from image but how are we supposed to extract the P from image and then we have a p andu
13:39
processing pipeline so we have our test image and we extract u w we call the W
13:47
in this equation down here by applying a wavel noiser plus a v filter and Main
13:53
and then we apply a zero mean by rows and columns to remove any periodic artifacts introducing during the com
13:59
acquisition process why we want to remove any periodic artifacts introduced
14:04
during camera acquisition process because they are not unique meaning that this device here that I'm using and my
14:11
smartphone May introduce similar periodic artifact that may ially correlate and generate false
14:17
positives now this is how we extract the pnu from a single image the image we
14:22
want to test how we are supposed to exract the p u from
14:28
um a set of image of a specific device to compose the camera fingerprint well
14:35
um in this case the camera fingerprint we are going to refer to it with the um letter K and the camera fingerprint is
14:43
composed by extracting the pru from each image taken with a specific device we
14:49
want to compare our pru with and then aage it together let's say using this
14:56
equation where W is the p and U of each of this image I is the gray scale
15:02
version of our um of the image from which we exract the pru and then we
15:07
divide it by the square of the um um what this the squared sum of the image
15:14
the ccale image from which we exract the P um here I put some questions um and
15:24
the questions are um why are we not using only one image to compose the
15:30
camera fingerprint well because more image the better representation of the
15:35
camera fingerprint we can have you can also try it in the exercise of the day to compose your camera fingerprint with
15:42
just um one single image and then maybe try two three and so on so
15:49
forth then the second question is are all image fine um well not really um
15:56
there are image that are more suitable to computer the camera fingerprint and others specifically images of blue sky
16:03
flat walls flat Skies cloudy skies and so on so for so images in which they are
16:09
well um illuminated they are bright but the um pixels are not born meaning that
16:16
the pixels are not set to a value equal to 255 255
16:21
255 or not completely dark or not completely full of textures are more
16:27
suitable to compose our camera fingerprint this is because our filters are
16:33
um works better in separating the contributes of the pru from those of the
16:39
image instead when we have textured images the pru got contaminated with
16:45
artifacts and well noise Source due to textures and other kind of um image
16:53
features so the take away here is that if you want if you have to
16:59
a camera fingerprint and you want to and you want the camera fingerprint to be um
17:04
the most accurate estimation in that case you need to use um flat images and
17:11
not images full of textures and and uh most likely images that preferably
17:18
images that are well illuminated and that are not too dark or um yeah they
17:26
are not dark or has too many Shadows then once you
17:33
have composed the camera fingerprint and you have extracted the pru of the image you want to test you want to check how
17:40
similar they are and to check how similar they are you're using the peak of correlation energy Ratio or BC which
17:47
are the cross coloration well the square cross coloration at the numerators
17:52
centered in the Sak so centered in the peak of correlations
18:00
divided uh well basically is the same uh the same measures but excluding the peak
18:07
of correlations you will see during the implementation of well during the the
18:13
coding part that actually you don't have to implement all these equations we already have implemented but just to
18:19
make you sure the peak of correlation energy is the ratio between the peak of correlations measured in the correlation
18:26
Matrix between the noise residuals and the cam fing F print and the um peak of
18:32
correlation excluding the actual Peak where you have the maximum correlations
18:37
between the um nose residuals and Camera fingerprint well pru and Camera
18:43
fingerprint in the correlation Matrix now um now that we saw um when uh
18:52
when the p is used so we know that is used versus attribution which are related to all those cases of privacy
18:59
viations in which having an image we want to identify the device used to uh
19:04
take that image and now that we um know why the pru is unique because it depends
19:11
by the manufacturing process of the sensor and how it can be instruct so a series of filter and now it can be
19:17
combined to compose the camera fingerprint to then compute the peak of correlation energy between the camera
19:23
fingerprint and the residuals um let's see how we can actually implement a code
19:30
able to exract the p u um compos the camera fingerprint and
19:36
um and computed the peak of correlation energy so before starting um the you
19:42
need to create a new virtual environment you need to create it as we did during our first laboratory and I also add in
19:51
the folder of laboratory 4 the requirements let's see I think that I have it here exactly um the requirements
19:59
for this laboratory uh and for the virtual environment of this labatory that are here maybe you don't need to
20:05
recreate a new a brand new um virtual environment but if yours is Raising
20:11
error just know that you need to create it with this um with these requirements um probably
20:19
when you finish to when you will finish to install these these requirements and
20:25
you are going to um try and using on your Jupiter notebook um vs code will
20:33
make you install an additional uh requirements to run your virtual environment on jupyter Notebook um just
20:42
install it so press okay and just install all right then so uh if we run
20:47
this code indeed what happens is that we take our image we uh separate this contribution from no sources and then we
20:54
exract the pmu from other no sources Maybe yes somehow is um the first step we are
21:02
doing is this one so here I is our image and F Well it comprehend all the
21:10
filtering process here we have both the wavel noiser plus defin filter in D
21:15
domain but let's say that whatever is concerned the W noiser and the def
21:21
filter in D domain is represented here by this um minus f i
21:29
all right so um once that we extract the P from our image the next step for our
21:35
source attribution task is to uh compose the camera fingerprint so to compose the
21:41
camera fingerprint we have we need to have a camera and then we need to have a set of image taken with this camera the
21:47
set of image taken with this camera are the one that I like that right now and
21:53
then uh we have get fingerprint and then uh well yes
21:59
um get fingerprint extracted be the noise from our set of images and then we
22:06
have the V future in the FD domain with the set specific value of
22:11
Sigma um where Sigma is the standard deviation of our um image from RGB to
22:19
gray uh so we're going to use to compose our camera fingerprint that here is represented at fingerprint so if we run
22:26
these cells you you can see that here it takes the first image with index zero
22:31
one and two and need compose our camera fingerprint once that we both have our
22:37
pru and Camera fingerprint what we can do is compute the PC to compute the PC
22:44
we need our image from well in our gray skate image that we are going to
22:50
multiply with the camera fingerprint this is to take into account any possible um let's say textures um
22:59
alteration of the pnu in our original image when we extract pnu also in the
23:05
camera fingerprint and then uh the first thing that we do is to compute the cross
23:10
correlations between our well the crosscorrelation Matrix between our um
23:15
p&u and Camera fingerprint here and finally what we do is compute
23:21
our PC so the computer PC using the equation of before which is the peak of
23:26
correlations in the cross correlation Matrix div divided by the um pick of
23:33
correlation excluding well the correlation the maximum correlation excluding the uh peak of correlation of
23:39
the numerator and if we run this C code what happens is that uh we can see that
23:45
actually the image we are testing this one pxxx um is actually taken with the same
23:52
device of the image P1 P2 and
23:57
B3 because its correlation is very large so we have a correlation of 200 um and
24:04
one a PC equal to 2001 um this function here so the MD the
24:12
one that I'm lighting right now the md. PC also show other values specifically
24:18
the P value so the probability of error concerning this PC value we have the
24:24
peak location which correspond to the coordinates on which the um maximum
24:31
correlation between the p u and the camera fingerprint has been retrieved the Peak High the probability
24:37
of f alarm which as you can see is very small and the probability of f alarm in
24:43
logarithm scales now um why you should be interested to know the peak locations
24:49
in which you computed the VC because if your image uh well if your camera
24:56
fingerprint and your pru has the same size um you're supposed to find the peak
25:01
of correlation in coordinate 0 0 which if you look very closely into this
25:07
plot that is also generated by this cell of of code you can see a very
25:14
large and narrow spike in correspondence of the coordinate uh well in this case
25:21
is a 0 0 but the axis here are somehow um I's say
25:26
broken in the case you are you are seeing it in correspondence of the
25:32
coordinates I believe that is zero and th000 but um actually is z 0 it's just a
25:39
plot that is a bit broken in terms of coordinates so if your pru and your
25:46
camera fingerprint has the same size you're supposed to retrieve this p on this coordinate so z z If instead your
25:54
pru is much smaller than your camera fingerprint then you need need to search for this peak of correlation that here
26:02
is located in0 for every possible shift occurring between the pru which has a
26:09
smaller size and the camera fingerprint so you need to let's say shift around
26:15
your p u over the camera fingerprint and compute for each of them the correlations and then on this plausible
26:22
shift you need to locate the maximum correlations well the peak of correlation um we will see some cases in
26:29
the next part of this uh of this class then the second things that I need
26:35
to say is that I say that we actually received a very large value of PC and so we can be quite sure that um the image
26:43
we were testing in this example were taken with the same device of the images we used to compose the camera
26:49
fingerprint um I'm not disagreeing with this but as we learn for the um water
26:56
marking challenge you also need to be well you cannot just say that what I said you cannot say it
27:04
that just because you see a very large value of PC then for sure this image was taken with that device uh remember
27:10
always to set a thresholds how you set the threshold using the rock carve and or other thresholding methodologies you
27:16
may know uh from other studies but always compare the measure you take with
27:22
the thresholds that take into account the probability of f alarm set at a specific um full positive rate and so on
27:29
and so forth all right so after this uh to
27:35
disclaimer uh the question is then what happens if an image was taken with a different camera so what we could expect
27:43
well we just going to run this code here for instance we are reading image imh z.
27:50
gpeg which is an image taken with another camera not the same one um used
27:56
to compose the camera fingerprint and what happens is that well the PC as you can see is expected as is expected to be
28:03
is much larger is much smaller sorry um here we have a value of minus 5.
28:11
5958 the probabil the P value is around 0.99 the pick location is over the same
28:18
so 0 0 but if you check this plot in correspondence of the same coordinates
28:24
uh so you compared this plot with the previous one you can see that you don't actually see a spike um in that location
28:32
the they are all similar values um so not really a big spike showing also that
28:39
the image was taken with the same device of the camera fingerprint uh the P I get much smaller indeed is minus
28:46
0.02 if we round it is 0.03 minus 0.03
28:52
while in this case is 0.01 if you R it this 0.02
28:59
um the P the probability of f alarm is 0.99 and the
29:04
logari well the probability of alarm on logarithmic scale is minus
29:09
0.0 uh 39 so this is somehow what you would expect in the case of an image not taken
29:17
with the same device of uh the camera used to compose the camera
29:23
fingerprint now this is more or less how things used to work on on the best case
29:29
scenario on an Ideal case however piu is affected by some kind of problems the
29:36
first problems um are related to his robustness is not very robust meaning
29:42
that even very small and um let's say innocent uh spal
29:49
transformation such as down scal with Factor 0.98 um can reduce the correlation
29:57
between the image pru and the camera fingerprint generating false
30:03
negative the second problem is that if an image pru and a camera fingerprint
30:09
are not perfectly align meaning that even if your p u is of 5 pixels smaller
30:16
than the camera fingerprint and you're not actually checking the correlation on
30:22
all possible shift occurring between the pru and the camera fingerprint um you may have you may
30:28
suffer a problem of false negatives and then well J by compressions of course if you compress an image you are going to
30:35
deteriorate the image but the p andu as well and you can Hender the pru
30:43
reability the solution to these problems luckily they exist um for the point
30:49
number one for well for problem number one and problem number two uh the solution consists in developing
30:55
algorithm to reverse this speci Transformations and will realign the pru
31:01
of the image with the camera fingerprint um while for the GP compression actually the only solutions
31:07
if you want to uh keep using the pru is to um use a different
31:14
filter with respect to the V filter in the FD domain in order to have a more robust estimation of the
31:22
pru now um but let's check each of these problem so we have the problem p new
31:28
versus special transformation um open parenthesis radi Corrections so what happens is that um
31:36
what happens when an image is R corrected well the image is R corrected using this model here the first
31:42
equations uh well the first and only the equations for this uh uh for these sections and what this equation
31:49
represents well R are the um radi distance from well is the distance
31:55
corresponding to the radi of of that a specific pixel to the center of our image so every time that we uh apply a r
32:04
correction we take the distance of a pixels from the center of the image so the radi corresponding to the distance
32:10
of that pixel from the center of our image and we are going to change the
32:15
length of this radi using that equation where Alpha is our rad correction
32:20
parameters so the one that modulate if we are going to move further from the
32:26
center or closer from the center with pixels uh while R is the actual distance
32:32
from the center of the image um if we use for instance well I
32:39
think that with this um equation a positive value of alpha we're going to
32:44
apply a bar Distortion meaning that when we have an original image we are going to obtain this bar bar Distortion image
32:52
but if we use a negative value of alpha we are going to obtain this Pion Distortion image now now if you see like
32:59
that you may think well then radal Distortion Corrections or uh how we call
33:05
it special transformation R Corrections um they are just moving around pixels so why this transformation
33:12
should affect our pru and the reason is that to each
33:17
pixels information there are also attached the uh p u
33:24
information so I pretended we are going to move these pixels um we are going to change the position
33:30
of the pixels and of the information of the pru related to those specifics
33:36
pixels and when we are trying to correlate this Radia corrected image
33:41
with a camera fingerprint of image that we are not ready corrected we are not finding in the same positions of the
33:48
pixels of the camera fingerprint the P information of the image we want to test
33:53
so we are decorrelating the P information with respect to the cam fingerprint
34:00
information then if we are going to check over here this uh cell of code and
34:07
we run it what we are doing is taking the camera fingerprint of a Canon SX 230
34:13
HS taken with um well composes of images taken with focal length equal to um 70
34:21
now uh why we have a camera fingerprint composed with just image taken with
34:27
focal length equal to 70 and the reason is that the r Corrections on our
34:32
smartone device the SLR camera and other device is applied in concert in concert
34:39
with the value of FAL length because if we use a large value of FAL length for instance equal to 70 probably this image
34:45
will be affected by pushes Distortion and the rad Corrections will compensate the pushen distortions remember that the
34:53
pincushion Distortion has this um these artifacts
34:58
here so the line moves towards this horizontal these um Horizon Lines of
35:04
these supposed to be horizontal Rines are let's say squeeze uh close to the
35:10
center and so the radal corrections in order to make them straight As in the case here of the original image we'll
35:17
apply a bar Distortion in order to rectify these lines however if we take an image for
35:24
instance image five which was taken you can check by I think that it was taken with a fuon length um equal to 4.3 or a
35:33
very small value of FAL length this image was affected by uh bar Distortion
35:38
meaning that the original image before applying the r Corrections looks like the bar Distortion Linea here and it was
35:47
applies up in cushions rad Corrections in order to rectify his lines so um this
35:53
means that there is a big distance from the pi VI um informations of the test
36:00
image and the P information of the camera fingerprint of the device of the can 200 um SX
36:09
230 so what we are showing here is that if we are going to compute the the PC
36:15
using this camera fingerprint and the um test image so this camera fingerprint
36:21
here that then aligned it and test image um I am five
36:29
what happens is that our uh PC will looks very small um actually it looks
36:36
equal well it it is equal to 80.5 the P locations is located to 0
36:43
because the we are in a perfectly unline case meaning that the p and the camera fingerprint has the same size I can
36:49
assure you that this image was taken with a Canon SX 230 but as you can see uh we have a very
36:56
small value of PC this is because the image used to compose the camera fingerprint of the Canon SX were Radia
37:01
corrected compensating the Pion Distortion with a barrel rad Corrections
37:07
and the image that we are testing were probably affected by bar distortions and were R
37:12
corrected using a um pink cushion rant Corrections then how can we uh recover
37:20
the prnu radi ability and increase this value of PC inverting either the rad
37:27
correction applied to the camera fingerprint or the rad correction applied to our test image in this case
37:34
we did it with our test image and we can run this script here I think that is
37:39
going well no this is just the function that we are going to use so to invert the radal corrections
37:46
what we need to do will be to move from cartisian coordinates to Polar cordance
37:52
because we are working with the distance of the pixels from the center of our image which is the
37:58
these R um [Music] distances then we need a functions that
38:04
represents our rad correction models that is this one here and then well a
38:10
function that apply um distort fct to our image and here we add either
38:18
the B Distortion the push distortions which well these functions basically Implement what we just saw in our
38:25
previous section and then also fun to apply a beginner interpolations in order
38:30
because when whenever you're are going to move pixels of an image there would be some empty spot so we need to re
38:36
interpolate them in order to don't have empty pixels all right so if we run this cell
38:44
of code as you can see here we are still going to read our camera
38:49
fingerprint and then we are going to compute uh let me see where it is we are
38:56
going to compute the PC um before the radal correction inversions applied to image five which
39:04
is the one that we compete before and then after specifically uh I can already
39:09
told you that the value used to R correct image was equal to minus
39:16
0.090 and indeed if we use this parameter minus
39:22
0.019 to invert the r correction in that case not even applied to the image but
39:27
we are inverting R Corrections um on the noise residual you can also do it on the
39:33
image but in this case it's much faster because you don't need to revert the r correction on each of the three channel
39:40
of the image you're just taking the image exract the ISP on you and then invert the r correction applied to the
39:45
pru because it's the same one applied to the to the whole image but anyway if we are using this
39:52
parameter you can see that before inverting the r Corrections the PC was actually equal equal to
39:58
18.5 while after inverting the rad Corrections applying um well actually
40:04
inverting a barrel um not applying a barrel Distortion because the r
40:10
correction applied to the image was a um up in cushions R Corrections our final
40:17
PC value is equal to 651.8rpm
40:24
[Music] what happens when an image is R corrected so the takeaway here is that
40:31
whenever an image is R corrected you need to invert these R Corrections before competing the PC now uh not
40:38
always you can know if an image was right corrected or not so if you have a very small value of PC you may first try
40:46
to invert the r Corrections or maybe you can try to invert any type of other type
40:51
of special transformations in order to try to increase um the final PC so is
40:56
some sort of of trial and there are leased on Theory then
41:03
[Music] um the next case no before moving to the
41:09
next case which is downscaling um in that case the other thing is that I knew
41:15
what was the value of uh used to that can be used to invert the right the
41:21
correction appli to our image and to its pru uh in the real work case scenario
41:26
you don't know it um so you need to infert it you need to infer it by using
41:31
predicting algorithms or by checking features of your image that may be
41:37
reconducted to uh a specific parameters of R Corrections you will have some
41:42
exercise and well you can experimentat using the pnu
41:48
um and uh this function for inverting R correction in order to guess what R
41:54
correction parameters was used to uh to R correct the image and so you need to
42:00
um you need to you need to infer to invert the r correction and maximize the
42:05
final PC value another problem and another
42:11
special transformation that may generate problems when applied to image and um
42:16
when we want to use the pru for Source attribution problems is the
42:23
downscaling um here I need to add you will have it on the final version I need
42:28
to insert the uh downscaling um downscaling model so even if right now I
42:34
don't see the um the downscaling equations uh we are going to we are
42:41
going to represent here you will see on your notebook um so even downscaling here
42:47
what happens is that whenever an image is downscaled in
42:53
this case is even more clear that is image pixels and been shift with respect to the original positions and as we
43:01
learn pixels of our test image in different position with respect those of
43:06
the camera fingerprint correspond to false negatives because they are not completely align they de correlate and
43:13
as a consequence we have very small value of PC and indeed this is what happens here so even here the PC is
43:20
equal to 21 in this case the function is even telling me that the camera
43:25
fingerprint and the image be you are not perfectly in line so here we are evaluating using the shift range uh all
43:33
possible shifts occurring between the p and the camera fingerprint in order to locate the maximum correlation the well
43:40
the peak of correlation uh value in our correlation Matrix uh
43:47
C now the solution in the case of non scaling is very similar with respect to the one we use in the case of the r
43:54
Corrections specifically um if an image was R corrected what happens is that um we
44:01
need to invert the rad Corrections if an image is down scale we need to upscale it so here what we are doing is just
44:10
using the rescale function of uh ski image.
44:16
transform applying a scaling value of uh one over 0.5 so we are going to
44:22
upscaling the image in order that it size becomes two times the one of the down scale
44:30
one and if we are doing so and we check is PC as we already observed in the case
44:37
of the r Corrections what happens is that uh well in the first case uh the argor is tell
44:44
me that the camera fingerprint and the image appear and you size are not the same so we are not perfectly align and
44:50
our PC is 21.18 in the case instead I upscale it
44:55
using k equal to 0.5 well we are not still perfectly in line because probably our p u is um a bit smaller with respect
45:04
to our camera fingerprint but our PC definitely increase equal and becomes
45:09
equal to 60 so even in the case of down scale image um you need to upscaling
45:16
them and these may be the cases of images on which we use the digital Zoom
45:21
every time that we use digital Zoom what we are doing is having our image in full resolution and then we are going to uh
45:29
zoom in a specific regions and then let's say upscale it to full resolutions
45:35
however this specific region is an upscaled version of a previous original image and so in that case we should
45:42
downscale that image in order to then check every possible shift that occur
45:48
between the pnu of the digital zoomed uh image and the camera fingerprint
45:55
and um find the peak of correlation
46:01
energy the second Cas well the the all right so um before moving on we saw that
46:09
the pru is mostly used for attribution we saw that the suffers of problems of false negatives we saw how we could
46:16
solve them or some of them uh by reversing this kind of transformation but as you can read here here you can is
46:27
actually being us it also for uh tempering detection why I say it can is
46:35
because um if you implement algorith using the P to check uh regions of image
46:41
that have been modified maliciously or where object has been spliced uh pru can
46:48
be effective since a certain point meaning
46:53
that if this image has been uh modified forged tampered uh and then
47:01
compressed post process and so on so forth you will be not able to use the
47:06
pnu to um Locate the temperat
47:11
regions however if uh the tampering is very weak then probably the P will still
47:17
be affected but let's see
47:22
um all right yeah here is what I what I wrote and what what is the solution in
47:28
the case your image has been modified by a pro or well probably modified and then
47:34
Double J compressed or postprocessed in any possible way uh for which your pru
47:41
is not more effective what is the solution if I still want to use the pru for tempering detection the solution is
47:47
not use the pru you will use another another techniques in the next labatory we will
47:52
see some of them all right so let's see uh for instance here
47:59
we have um how can how the pru can be used for temperary detections um in this
48:05
case we have a camera fingerprint of a Canon ,200 and here we have an image called
48:13
IMG 0.047 um this image we presume it was
48:18
tampered so what we are doing is to extract the pru um and then we are going to well
48:26
first of all check if the image well if the pru and the camera fingerprint are perfectly Allied or not and then um what
48:34
we are doing is check the PC um between the p u and the camera
48:42
fingerprint however we are checking it uh not globally so we are not taking our
48:47
pru our camera fingerprint computer cross correlation Matrix and check is PC
48:53
has a single value but we are going to subdivide our pnu and our camera fingerprint into blocks and then checks
49:01
the PC value um well the local PC value of each overlapping blocks of the pru
49:08
and the camera fingerprint so the results is this one as you can see here from the detection
49:15
PC map we have this uh yellowish and light blue value of blocks of this PC
49:24
map because here we have the local value of each well the local PC value of each
49:30
of these blocks computed between the p&u and the camera finger print however here
49:36
we have this shape very over shape uh with very low value op in and then if we
49:44
check this image here and we check our temporate image indeed we can see that here in this garage there were um Super
49:52
Mario Bros so yeah the image was DED then you will have an exercise in which which you will take the same image here
49:59
and you will try to apply J compressions and you will see that this PC map will
50:04
just become worse and worse up to the point that if you need to rely just on this PC
50:10
map um to find where an object uh well to find
50:16
if an object was or not um spliced on the image and if the image was tampered
50:22
of not um you will probably see that this image which is actually Ted is
50:28
pretty obvious um is actually an original one and Super Mario is actually
50:34
in the garage so just to resume even here for tempering detection pru can be
50:39
used but is very weak against every kind of image processing geg and whatever you
50:45
use to attack the other image during the challenge the pnu is very very
50:51
weak um what we are representing here is the PC map where the PC value is not
50:57
computed by just taking the P the camera fingerprint and compute a single value
51:02
of PC but we have divided the pru into blocks and then we have divide the
51:09
camera fingerprint into blocks and we are Computing the PC value between each blocks of the pru and the camera
51:16
fingerprint this is why you have this very blocky structure in this detection
51:22
PC map and um this is also why some blocks are brighter than others because
51:28
the blocks Associated to yellowish value well to yellowish colors sorry
51:34
correspond to high correlation so high PC value while the blocks that are dark blue correspond to low correlation value
51:42
meaning that this um well the blocks of this um corresponding to this region in
51:48
the original image at least in this case we can presume that they were that they
51:53
were modified or tampered or forged by a second which make this image
52:01
fake now to conclude and then move to a very first second part but is very
52:08
theoretical um the p u so uh we saw that the pru can be in many police
52:16
investigation task specifically in the case of s attributions and then
52:21
sometimes we can use it also in um for temporary detection but mostly for
52:28
Source attributions as temporary detection using the pnu is not
52:34
reliable now H today is still used in many of the cases of s attribution even
52:40
considering that um is affected by many problems some problems are related to
52:46
false negatives meaning that if you are applying even very uh weak spal
52:53
transformation to your image it affects the thepu generating false negatives but
52:59
in the past few years and here I attached a paper let's see if we can
53:05
open it there we go in the past few here it
53:10
was observed that another problem affecting the pru on new devices is
53:15
false positives now why the pru is affected by false positive problems here is the paper if you curious to read the
53:22
reason seems to be attributed to the new camera acquisition process processing techniques that involves while more
53:29
sophisticating image announcement processing also involving the use of the ey so what happens is that if we look at
53:36
this plot of which one of them I was attou in our notebook um you can see
53:42
here that the yellow dots correspond to True positive while the red dots
53:47
correspond to True negatives but let's look for instance for the iPhone 11 Pro
53:53
well here we have um let's zoom in here we have this is the
53:59
iPhone 11 Pro there we go here we have our um red dots corresponding to true
54:06
negative here we have our Green Dots corresponding to True positive and then here we have a bunch of red dots
54:13
corresponding to through negative but if we set a threshold equal to for a PC
54:18
equal to 60 all these red dots are on the wrong sides um well on well yeah are
54:26
on the on the wrong side of the thresholds uh and they will be all detected as true positive but actually
54:34
they are true negatives so they are actually uh false positives the iPhone 11 Pro is one of
54:40
the devic and the iPhone 11 yeah the iPhone 11 Pro is one of the device
54:45
affected but we can see that who in the iPhone 7 plus we have some case of false negatives and here we have the UAA P20
54:56
Pro we can see that we have many dots here also in the mate 20 Pro corresponding to
55:03
false positive and related to these uh New Image anouncement techniques that
55:09
are used during the camera acquisition process so the intuition is that uh this is happening because uh
55:16
this new image announcement processing techniques introduce some nonunique
55:23
artifacts these nonunique artifacts going going and uh let's say they
55:29
they're not grading but they they are mixed with the
55:34
pnu we exract using the puu extraction pipeline we saw in this laboratory so they contaminate the
55:41
p and they are so resilient and so much that when we are going to compute the PC
55:48
between an image taken with one let's say P20 Pro and so device a which is a
55:55
P20 Pro and a device B which is also P20 Pro we are not actually correlating the
56:01
pnu but we are correlating the non unique artifacts so this is the
56:06
explanation as you can see these are UA um but we have bunch of other devices uh
56:13
let's just look here for instance we have the Galaxy X10 um here we have two kind of Galaxy
56:20
S10 because you have two kind of cameras the 12 megapixels and the 16 megapixels and then we have S10 plus also very
56:28
affected by F positive the Galaxy a15 of which we found our Solutions in one of
56:34
our last paper that I going to show you in a few minutes but then you well in
56:40
the case of the Nokia uh pure view more than false positive it seems a problems
56:45
related to how they set the thresholds indeed the two groups are are very well
56:52
separated so maybe by setting another kind of thresholds the problem is solved but then we have the xiaomi redmi Note
56:59
7 and well and also we have a bunch of DSLR camera look at here um on which
57:08
they softer a lot about F posting now some of these devices some of
57:14
problem of these device with these devices have been solved for instance for the iPhones um has been observed
57:20
that the problem were related with the bouquet mode so if you need to compose a camera fingerprint don't include images
57:26
taken uh in portate mode or using the bouquet because the bouquet is indeed a
57:32
source of nonique artifacts meaning that you going to extract the P but instead you're going to extract just the
57:37
artifact related to B and they highly correlate in the case of a couple of
57:44
other devices that I'm going to show you right now instead the reason we're a
57:50
little bit different so this is a paper that we recently published
57:57
um and as you can see here well for instance for the xiaomi M9 the problem
58:04
um initially was related to the fact that uh the image were watermarked with
58:09
this uh visible Watermark and this visible Watermark being always the same
58:14
highly correlate generating these false positives um so this was a reason not
58:22
really related to Unique artifacts but to other kind of uh image processing
58:27
techniques then for the let me see let me
58:33
see yeah this is also the xiaomi min9 also in the case of the xiaomi mi9 the
58:39
work set of image um that were um well not just Watermark but also mislabeled
58:46
in the data set of uh the previous paper I show you so this was another problem we
58:51
discovered um which are this one so there was this guy can picture of his
58:57
foot with um weird shoes and what were mislabeled well labeled has two
59:03
different devices but instead they were uh the same device then we have cases concerning the
59:11
Moto E5 and DS CX where we have the same problems due to mislabeling but then
59:19
more curiously there were this cases about the Samsung A50 which if you remember was another device of the PS
59:25
paper that was severely affected by false positives in which um initially we
59:32
thought the problems of false positive were related to Boke but when we remove Bou images from the composition of the
59:40
camera fingerprint um the FSE positive didn't get so the number of f posi
59:46
didn't get smaller but it just increased instead we learned that um the images
59:52
showing less problems in terms of FSE posit positives were those exposing some
59:59
spikes close to the center of their DFT magnitude so the discrete the magnitude
1:00:06
of their DFT transformation discrete fly transform which as you can see let me
1:00:12
zoom in here they expose these spikes so this
1:00:18
somehow indicates that images showing these kind of spikes are less affected by non unic artifacts and by exploiting
1:00:26
information as you can see here we kind of solve the problems in terms of false positive of this Samsung
1:00:32
A50 instead for um DSLR camera well and for many um here we have
1:00:40
for instance the DSLR camera and for many camera devices here we discover
1:00:47
that many of these image were actually not post processed by the in camera device but um postprocess out camera uh
1:00:55
and this generate many problems in terms of false negatives um so even here many
1:01:01
image were R corrected and so by reversing the r correction we will have to better separate true negative from
1:01:07
True positive and improve the results we show in the previous paper so summarizing this is just to say that
1:01:15
the pnu yes is still used very often uh
1:01:20
in many practical cases is affected by problems that ones need to take into account problems related to false
1:01:27
negatives and problem related to false positive and now there is a large research on problems related to false
1:01:33
positive that need to be addressed we want to keep using the pru um even because a couple of years
1:01:42
ago and uh with this I will conclude then my presentations um a new P was presented
1:01:51
um to the mul media forening Community a p you called the noise print now what is
1:01:57
the main difference between the p and noise print the difference is that noise print was estimated not using signal
1:02:03
processing filter so the wavelet and the V filter but using a
1:02:08
CNN uh now in what does it consist and we have it
1:02:17
here so I'm going to I prepare a very short presentation in what does the uh SN
1:02:25
Sprint assist well it's a deep learning based camera fingerprint
1:02:30
estimator um with respect to the pru is partial transformation insensitive
1:02:36
meaning that if you can rad correct an image don't scale it applying any type of spal transformation but this is not
1:02:43
going to affect the estimation of this p u let's say estimated with no Sprint and
1:02:51
is very effective with respect uh to the pru in case of tempering detections and
1:02:56
we will see some cases and um presents promising results but is uh still less
1:03:03
affected at the pnu for device identification and we will arrive to this
1:03:08
point so the intuition is that the pnu is a noise residual so is a noisy source
1:03:15
and there many problem to special Transformations and these problems um
1:03:20
relies on is because P relies on Signal processing so Thea that the auor had was
1:03:26
let's use a deep learning architecture to estimate the pnu so they start from
1:03:32
the DN CNN uh which is a gaou and the noiser um presented in 2017 I think and
1:03:39
this gaou and the noiser basically consist in taking a noising image extracting is residual and then well um
1:03:46
subtracting the residual to the noising image in order to have a noisess
1:03:52
image so what did dncnn wanted to do was to start from uh well have a ground
1:03:58
through during training have a ground through image then have a noisy image and the structor the the noise at image
1:04:04
which is as close as possible to the ground what the auor of noise print instead wanted was to take the SIM
1:04:12
network instead of obtaining the noisess image just extracting the noise from an
1:04:19
image and exract the noise so that if two well if the noise of um images is
1:04:25
taken with the same camera well if we have two noise extracted from image taken with the same camera they had to
1:04:33
exhibit a ukan distance which is as close as possible to zero because we want the two noisy estimation to be um
1:04:41
as similar as possible on the other hand if we have um two image that were taken
1:04:48
with two different cameras we want that the ukan distance between their noise
1:04:54
residual is as large as possible and so they train the network under these
1:05:00
rules here for instance well that there are additional details that they used
1:05:06
the batch of 200 patches of size 48 * 48 TR the network each patches each batch
1:05:13
is divided in 15 groups of four patches the patches of each group came from the same camera
1:05:19
model um and they're croped with the same special coordinate this is because
1:05:24
they want to resemble all these um pru features that if you have uh two online
1:05:31
images or two online crops taken from this image they need to exhibit an eye correlation while if they are taken from
1:05:37
two different special position they should
1:05:43
not and the goal indeed is to capture unique feature belonging to the camera
1:05:50
models now during training as I said they are checking the distance so if R1
1:05:55
is the residual extracted well r r e let me see
1:06:00
if I can have a pointer uh there we go if re e is the
1:06:06
residual extract from image e that um was taken with the same camera of the
1:06:13
residual RJ extracted from image J then this uh distance here is supposed to be
1:06:19
as closest possible to zero um so that the soft ma sof Max functions will
1:06:27
correspond well to a value as close as possible to the label one in the case of
1:06:33
uh match and zero in the case of mismatch all right um additional
1:06:43
information about the here we have also additional information about how the loss was computed specifically how the
1:06:50
at the beginning the distances used to train a noise print where estimated the
1:06:55
red lines refer to the distance computed between the residuals of patches of the same group well specifically of the same
1:07:02
um camera models the blue lines referring to the distance computed between the residuals of patches not of
1:07:09
the same group and so not of the same camera model why they did so they did so
1:07:14
in order to have um much more cases related to the mismatch than to the
1:07:19
match has they proven has it proven to be optimal for training of this network
1:07:26
the red line distance will have labels equal to one indeed to the case match
1:07:31
and the blue lines distance will have labels equal to zero to the mismatch cases and then in this last slide
1:07:39
basically just summarizing what I said at the beginning of this presentation well of this of this um of the section
1:07:45
related to how uh noise bring this Trin specifically to what the red lines
1:07:52
correspond the blue lines corresponds and why they use the distances when groups and distances computed both from
1:08:00
um patches of the same device or groups and other devices and why they use this
1:08:06
combination basically is to have a much larger number of samples related to the
1:08:11
mismatch cases than to the uh match cases has they proved that this was
1:08:17
improving the whole training of the network now uh the question is how this
1:08:23
network is actually working when um tested on similar cases has the
1:08:29
pnu and the truth is that is working pretty decently but not as good in the
1:08:36
optimal scenario has well in the ideal scenario has the P indeed if you see
1:08:42
these confusion matrices here where they use three uh well they use six device of
1:08:49
three camera models the Nikon d70 D200 and the smartphone um OnePlus
1:08:57
we can see that the diagonal expose um a good number for the case of noise Sprint
1:09:03
of true positive cases but they also have some false positive they have a
1:09:09
much larger number with respect to the pru of false positive cases uh
1:09:14
specifically between devices of the same camera models for instance A1 and A2
1:09:20
which are Nikon d70 we can see that we have a large number of those positive between A2 and A1 and A1 and A2
1:09:29
similarly we can see well similarly B for B1 and B2 but then if we move to C1
1:09:36
and C2 we can see that for the smartphone OnePlus this FSE positive are actually
1:09:43
much larger a much larger number with respect to um C1 and C2 in the case of
1:09:49
the pru the good things however is that the P the this noise Brint techniques um is
1:09:58
um proved to be very useful and very effective um in the case of temporary
1:10:04
detections indeed the we respect to the pru um is much more robust through JP
1:10:09
compressions and any kind of another kind of image processing and indeed as you can see here from this heat map was
1:10:17
pretty um effective in localizing a different kind of image manipulation
1:10:24
tampering and Forger this is because uh well it extract um
1:10:31
deep um features using a neural networks instead Pingu exract features uh using
1:10:38
signal processing filters which um in presence of tampering also apply with AI
1:10:46
and so other deep neur networ techniques they are not able to capture and to distinguish features introduced by an AI
1:10:53
to the real image the other things is that um well the
1:11:00
other things before concluding is that the uh noise print however cannot be used in courtroom cases or better it can
1:11:07
just use as source of investigation and not source of proof at least in the
1:11:12
European Union that is where we are living and this is because um European
1:11:18
regulations uh we cannot use in our code room algorithms that cannot uh that are
1:11:24
not fully explainable and reproducible um which basically let's
1:11:30
cluster all the algorith using the AI such as nint nevertheless even um
1:11:39
nevertheless this kind of Technologies proved and keep proving to be very
1:11:44
effective specifically on in the case of temporary detection so in the future something will for sure change instead
1:11:52
concerning what we mentioned for the um device identifications is true that the
1:11:58
software of FSE positive in the case we have the same camera brand but it also have many Advantage specifically in the
1:12:05
case of spal transform transformed images as is much faster in um well
1:12:11
returning an answer if an image was taken or not with the same device because it doesn't need to invert any
1:12:18
kind of spal transformation um one last problems about noise print is that couple of
1:12:25
years ago I Was preparing a collab but this year uh is not possible anymore because they're not supporting anymore
1:12:31
there GitHub repository the kup code was using python 3.5 up to two years ago we
1:12:39
were able to use Python 3.7 but now collab is refusing to work
1:12:44
with python 3.7 so this is another the problems about this deur networ techniques that
1:12:50
they may be very useful today um but they cannot be us it anymore or at least
1:12:56
we need to implement it from scratch all right and uh with this I
1:13:01
conclude my presentation I left uh some exercise of
1:13:07
which you will find the code with the registration of these uh of these
1:13:12
lectures and um you will also find the solution so we see each other in our
1:13:18
next labatory thank you
